This is my daily logs where I will be mapping down the timeline of this project on a daily basis. This will be updated everyday with everyday tasks.

TODO for 6/8 - 6/14: Get a crawler working to get PDF articles from online journals
Day 1 (6/8):
	- Use Jsoup to crawl
	- After spending the whole day trying to understand JSoup, I finally got one working. All my output for today are all saved in src/samplePDFExtract/output.txt
	- I got 17 total case reports from https://www.sciencedirect.com/journal/trauma-case-reports/vol/25 -- This is volume 25 of year 2020
	- Tomorrow, I will try to figure out how to parse through mutliple URLs and extract more articles that way
Day 2 (6/9):
    - After using JSoup for crawling, I was able to figure out how to parse in multiple URLs at once.
    - The performance did slowed down a bit and I kind of expected that.
    - The results can be found on the main folder -- src/main/output.txt
        - Currently, I was able to crawl 6 different urls from the same publishing company and extracted 107 total PDF files -- including the Editorial Board
        - During text cleaning, I need to figure out a way to put all extracted PDFs into a CSV with two columns -- Title and URL Links
Day 3 (6/10):
